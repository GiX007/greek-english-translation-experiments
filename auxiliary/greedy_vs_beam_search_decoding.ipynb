{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Greedy decoding and beam search are two common strategies for generating sequences from autoregressive models.\n",
        "Greedy decoding selects the most probable token at each step, while beam search keeps multiple candidate sequences to explore better global solutions."
      ],
      "metadata": {
        "id": "6EaYOE9zyxJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "fLsWd7Zryz3c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy vocabulary\n",
        "vocab = [\"<SOS>\", \"I\", \"you\", \"love\", \"hate\", \"cats\", \"dogs\", \"<EOS>\"]"
      ],
      "metadata": {
        "id": "aKw7tHrPyz0S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EoovzvJOxOpG"
      },
      "outputs": [],
      "source": [
        "# A fake model\n",
        "def toy_model(prev_token):\n",
        "    probs = {\n",
        "        \"<SOS>\": {\"I\": 0.6, \"you\": 0.4},\n",
        "        \"I\": {\"love\": 0.51, \"hate\": 0.49},\n",
        "        \"you\": {\"love\": 0.9, \"hate\": 0.1},\n",
        "        \"love\": {\"cats\": 0.5, \"dogs\": 0.5},\n",
        "        \"hate\": {\"cats\": 0.9, \"dogs\": 0.1},\n",
        "        \"cats\": {\"<EOS>\": 1.0},\n",
        "        \"dogs\": {\"<EOS>\": 1.0},\n",
        "    }\n",
        "    return probs.get(prev_token, {\"<EOS>\": 1.0})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding\n",
        "def greedy_decode():\n",
        "    sequence = [\"<SOS>\"]\n",
        "    score = 0.0\n",
        "\n",
        "    while True:\n",
        "        prev = sequence[-1]\n",
        "        next_probs = toy_model(prev)\n",
        "\n",
        "        # Pick highest probability token\n",
        "        next_token = max(next_probs, key=next_probs.get)\n",
        "        score += math.log(next_probs[next_token])\n",
        "\n",
        "        sequence.append(next_token)\n",
        "        if next_token == \"<EOS>\":\n",
        "            break\n",
        "\n",
        "    return sequence, score"
      ],
      "metadata": {
        "id": "4cHQse58zCjs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_seq, greedy_score = greedy_decode()\n",
        "print(\"Greedy output:\", greedy_seq)\n",
        "print(\"Greedy log-prob:\", greedy_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSi7GIIbzUKn",
        "outputId": "5fc1851a-bed8-4c05-88aa-48aa6b35aa51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy output: ['<SOS>', 'I', 'love', 'cats', '<EOS>']\n",
            "Greedy log-prob: -1.8773173575897015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beam search\n",
        "def beam_search_decode(beam_size=2):\n",
        "    beams = [([\"<SOS>\"], 0.0)]\n",
        "\n",
        "    while True:\n",
        "        new_beams = []\n",
        "\n",
        "        for seq, score in beams:\n",
        "            prev = seq[-1]\n",
        "            if prev == \"<EOS>\":\n",
        "                new_beams.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            for token, prob in toy_model(prev).items():\n",
        "                new_seq = seq + [token]\n",
        "                new_score = score + math.log(prob)\n",
        "                new_beams.append((new_seq, new_score))\n",
        "\n",
        "        # Keep top-k beams\n",
        "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "        # Stop if all beams ended\n",
        "        if all(seq[-1] == \"<EOS>\" for seq, _ in beams):\n",
        "            break\n",
        "\n",
        "    return beams"
      ],
      "metadata": {
        "id": "3ZqDdFvVzX-U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beams = beam_search_decode(beam_size=2)\n",
        "for seq, score in beams:\n",
        "    print(\"Beam output:\", seq, \"log-prob:\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYhNiHIrzrD1",
        "outputId": "18bcedf1-00a7-418d-dfc6-a78d44e49a2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam output: ['<SOS>', 'you', 'love', 'cats', '<EOS>'] log-prob: -1.7147984280919264\n",
            "Beam output: ['<SOS>', 'you', 'love', 'dogs', '<EOS>'] log-prob: -1.7147984280919264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CCompare results\n",
        "print(\"Greedy:\", greedy_seq)\n",
        "print(\"Best beam:\", beams[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_DyeEISzyzo",
        "outputId": "8f6fe018-70b0-4b53-c347-e67af6a25f9e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy: ['<SOS>', 'I', 'love', 'cats', '<EOS>']\n",
            "Best beam: ['<SOS>', 'you', 'love', 'cats', '<EOS>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy decoding is fast but short-sighted, making locally optimal decisions that can lead to suboptimal sequences.\n",
        "\n",
        "Beam search trades speed for quality by keeping multiple hypotheses, often producing more fluent and higher-probability outputs."
      ],
      "metadata": {
        "id": "EaDOw32q0HLO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgA0zzzY0Pz5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}