{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Common machine-translation evaluation metrics are demonstrated using simple, controlled examples.\n",
        "The goal is to build intuition for how CER, WER, and BLEU behave under different prediction errors."
      ],
      "metadata": {
        "id": "o_6Qn9gquy6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPrl56pru_82",
        "outputId": "51bf62c2-983c-4a41-f99d-ef065939007c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/983.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.9/983.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kbh89KLluncw"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.text import CharErrorRate, WordErrorRate, SacreBLEUScore\n",
        "from torchmetrics.text import SacreBLEUScore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perfect prediction\n",
        "predicted = [\"this is a test\"]\n",
        "expected  = [\"this is a test\"]\n",
        "\n",
        "cer = CharErrorRate()(predicted, expected)\n",
        "wer = WordErrorRate()(predicted, expected)\n",
        "bleu = SacreBLEUScore(smooth=True, tokenize=\"13a\")(predicted, [[expected[0]]])\n",
        "\n",
        "print(\"CER:\", cer.item())\n",
        "print(\"WER:\", wer.item())\n",
        "print(\"BLEU:\", bleu.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuW8MAjou0Qw",
        "outputId": "0fc8528f-f2d0-4d5e-ee4d-e24b2176786b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CER: 0.0\n",
            "WER: 0.0\n",
            "BLEU: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one word wrong\n",
        "predicted = [\"this is a demo\"]\n",
        "expected  = [\"this is a test\"]\n",
        "\n",
        "cer = CharErrorRate()(predicted, expected)\n",
        "wer = WordErrorRate()(predicted, expected)\n",
        "bleu = SacreBLEUScore(smooth=True, tokenize=\"13a\")(predicted, [[expected[0]]])\n",
        "\n",
        "print(\"CER:\", cer.item())\n",
        "print(\"WER:\", wer.item())\n",
        "print(\"BLEU:\", bleu.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk9g5ou3u0N1",
        "outputId": "ad8af33c-1294-421d-8402-1a5085cb8f79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CER: 0.2142857164144516\n",
            "WER: 0.25\n",
            "BLEU: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same words, different order\n",
        "predicted = [\"is this a test\"]\n",
        "expected  = [\"this is a test\"]\n",
        "\n",
        "cer = CharErrorRate()(predicted, expected)\n",
        "wer = WordErrorRate()(predicted, expected)\n",
        "bleu = SacreBLEUScore(smooth=True, tokenize=\"13a\")(predicted, [[expected[0]]])\n",
        "\n",
        "print(\"CER:\", cer.item())\n",
        "print(\"WER:\", wer.item())\n",
        "print(\"BLEU:\", bleu.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7MM94USvcZI",
        "outputId": "23ddba51-28e8-4701-fff3-ea43e68d0d00"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CER: 0.2857142984867096\n",
            "WER: 0.5\n",
            "BLEU: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# completely wrong prediction\n",
        "predicted = [\"this example is incorrect\"]\n",
        "expected  = [\"this is a test\"]\n",
        "\n",
        "cer = CharErrorRate()(predicted, expected)\n",
        "wer = WordErrorRate()(predicted, expected)\n",
        "bleu = SacreBLEUScore(smooth=True, tokenize=\"13a\")(predicted, [[expected[0]]])\n",
        "\n",
        "print(\"CER:\", cer.item())\n",
        "print(\"WER:\", wer.item())\n",
        "print(\"BLEU:\", bleu.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MGrRpzzv7SU",
        "outputId": "318b051f-da26-492e-afca-108900e9f9b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CER: 1.0714285373687744\n",
            "WER: 0.75\n",
            "BLEU: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple sentences (corpus behavior)\n",
        "predicted = [\n",
        "    \"this is a test\",\n",
        "    \"machine translation is hard\"\n",
        "]\n",
        "\n",
        "expected = [\n",
        "    \"this is a test\",\n",
        "    \"machine translation is difficult\"\n",
        "]\n",
        "\n",
        "cer = CharErrorRate()(predicted, expected)\n",
        "wer = WordErrorRate()(predicted, expected)\n",
        "bleu = SacreBLEUScore(smooth=True, tokenize=\"13a\")(predicted, [[e] for e in expected])\n",
        "\n",
        "print(\"CER:\", cer.item())\n",
        "print(\"WER:\", wer.item())\n",
        "print(\"BLEU:\", bleu.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxPDEfGQwF0Q",
        "outputId": "c44158cf-8875-471b-e32b-ad0939b3b43a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CER: 0.19565217196941376\n",
            "WER: 0.125\n",
            "BLEU: 0.7952707409858704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CER measures character-level edit distance.\n",
        "\n",
        "WER measures word-level substitutions, insertions, and deletions.\n",
        "\n",
        "BLEU measures n-gram overlap and is highly sensitive to phrasing and word order, especially on small samples, reaching its maximum value only when predictions exactly match the reference."
      ],
      "metadata": {
        "id": "HNKfkK5FwPnF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eN_IndQ2vu_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}