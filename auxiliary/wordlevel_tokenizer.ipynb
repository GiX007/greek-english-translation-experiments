{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The basics of building and inspecting a simple\n",
        "WordLevel tokenizer using Hugging Face Tokenizers, based on a small dummy dataset.\n",
        "It follows the official Hugging Face Tokenizers quick tour:\n",
        "https://huggingface.co/docs/tokenizers/quicktour."
      ],
      "metadata": {
        "id": "WMQ4LKjLbUmF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UN-tTHEKWlTa"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_ds = [\n",
        "    {\"translation\": {\"en\": \"hello world\", \"el\": \"γειά σου κόσμε\"}},\n",
        "    {\"translation\": {\"en\": \"hello there\", \"el\": \"γειά σου\"}},\n",
        "    {\"translation\": {\"en\": \"machine learning is fun\", \"el\": \"η μηχανική μάθηση είναι ωραία\"}},\n",
        "]"
      ],
      "metadata": {
        "id": "Onu9STkyWoFz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "  for item in ds:\n",
        "    yield item[\"translation\"][lang]"
      ],
      "metadata": {
        "id": "PKG-O8AXWtzO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(get_all_sentences(dummy_ds, \"en\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jv8BdIAWtwJ",
        "outputId": "993af83a-53af-4cf2-872a-7dc21237818b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello world', 'hello there', 'machine learning is fun']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(get_all_sentences(dummy_ds, \"el\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJwAaAl2XQML",
        "outputId": "65dfbc86-4005-4d72-bb4c-3fa34394045a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['γειά σου κόσμε', 'γειά σου', 'η μηχανική μάθηση είναι ωραία']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a WordLevel tokenizer (1 word → 1 token)\n",
        "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "LRFflptmXSnW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVMwjPcJYH_I",
        "outputId": "9fbf77bb-c61f-4225-ba8e-b984e7cd073e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[], normalizer=None, pre_tokenizer=None, post_processor=None, decoder=None, model=WordLevel(vocab={}, unk_token=\"[UNK]\"))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test it (this produces error as no tokens added yet)\n",
        "tokenizer.encode(\"hello\").ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "hEVASSS_YJgM",
        "outputId": "0ac20ea1-736b-45b7-ae4a-bd9f922eef36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "WordLevel error: Missing [UNK] token from the vocabulary",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2476427141.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: WordLevel error: Missing [UNK] token from the vocabulary"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add a pre-tokenizer (splits text before encoding)\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "metadata": {
        "id": "Q63H25-bYcdS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"hello world\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMi0GaegYcaW",
        "outputId": "ff11af36-0cde-4d7e-ae76-87798d553af8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hello', (0, 5)), ('world', (6, 11))]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the trainer\n",
        "trainer = WordLevelTrainer(\n",
        "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], # tokens added to vocab\n",
        "    min_frequency=2, # drop rare words, e.g., if 'hello' appears twice, keep it, if 'word' appears once, drop it (becomes [UNK])\n",
        ")"
      ],
      "metadata": {
        "id": "5h5uHArtYMAf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucHRul7uZjEp",
        "outputId": "69123608-4eab-4e5d-9c80-ebe79052baef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordLevelTrainer(WordLevelTrainer(min_frequency=2, vocab_size=30000, show_progress=True, special_tokens=[AddedToken(content=\"[UNK]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[PAD]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[SOS]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[EOS]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True)], words={\"is\":1, \"hello\":2, \"world\":1, \"learning\":1, \"machine\":1, \"there\":1, \"fun\":1}))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the tokenizer from our text\n",
        "tokenizer.train_from_iterator(\n",
        "    get_all_sentences(dummy_ds, \"en\"),\n",
        "    trainer=trainer\n",
        ")"
      ],
      "metadata": {
        "id": "oeo2GmVAZbT1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the vocab\n",
        "tokenizer.get_vocab()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVcL5582ZiQ3",
        "outputId": "1e4926ac-10f2-43b1-d657-b3b7907b26d8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[PAD]': 1, '[UNK]': 0, '[SOS]': 2, 'hello': 4, '[EOS]': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode exampple 1\n",
        "tokenizer.encode(\"hello\").ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbC8SAl5Zzy-",
        "outputId": "0e7100f2-84b3-48b3-8a51-cf863daee096"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode example 2\n",
        "tokenizer.encode(\"hello world\").ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxqXy9OAZtA5",
        "outputId": "02660125-3e8b-4b16-e6b9-77738d64f14c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xggXQGZbZ7Pi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}